# This is the name of the workflow, which will be displayed in the GitHub Actions tab.
name: QA Test General ‚Äî Crawler + Matrix

# Principle of least privilege for GITHUB_TOKEN
permissions:
  contents: read

# Manual trigger only (no cron) to avoid costs and automatic runs
on:
  workflow_dispatch:
    inputs:
      url_offset:
        description: 'N√∫mero de URLs a saltar (ej: 30 para analizar el siguiente lote)'
        required: false
        default: '0'
      url_limit:
        description: 'L√≠mite de URLs a analizar en esta corrida (opcional; sobreescribe MAX_URLS)'
        required: false
        default: ''
      analyze_all:
        description: 'Analizar TODAS las URLs encontradas (ignora l√≠mites). Puede tardar bastante.'
        type: boolean
        required: false
        default: false

# Ensure only one run at a time
concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: true

jobs:
  # Job 1: Discover all internal URLs using a robust crawler.
  discover-urls:
    name: "Paso 1: Descubrir URLs (Crawler)"
    runs-on: ubuntu-latest
    outputs:
      # The output is a JSON string array of URLs for consumption by matrix strategies.
      url_matrix: ${{ steps.make_matrix.outputs.urls }}
      total_urls: ${{ steps.count_all.outputs.total_urls }}
    steps:
      - name: "Sanity check BASE_URL"
        run: |
          if [ -z "${{ vars.BASE_URL }}" ]; then
            echo "Error: BASE_URL no est√° definida."
            exit 1
          fi
      - uses: actions/setup-node@v4
        with:
          node-version: 20
      - name: "Install Linkinator"
        run: npm install -g linkinator@4
      - name: Ensure jq
        run: sudo apt-get update && sudo apt-get install -y jq
      - name: "Crawl site, filter, and limit URLs"
        id: crawl
        env:
          # Define MAX_URLS in Repo Settings > Secrets and Variables > Actions to control scope.
          BASE: ${{ vars.BASE_URL }}
          LIMIT: ${{ vars.MAX_URLS || 30 }}
        run: |
          set -e
          # Entradas de ejecuci√≥n
          OFFSET=${{ github.event.inputs.url_offset || 0 }}
          REQ_LIMIT='${{ github.event.inputs.url_limit }}'
          ANALYZE_ALL='${{ github.event.inputs.analyze_all }}'

          BASE_URL_NO_SLASH="${BASE%/}"

          # Rastreo (no falla si hay links rotos)
          linkinator "$BASE_URL_NO_SLASH" --recurse --silent --timeout 10000 --format JSON > crawl.json || true
          # Si no hay JSON, coloca uno vac√≠o para que jq no falle
          test -s crawl.json || echo '{"links":[]}' > crawl.json

          # L√≠mite efectivo: prioridad al input; si vac√≠o, usa MAX_URLS
          if [ -n "$REQ_LIMIT" ]; then
            EFFECTIVE_LIMIT="$REQ_LIMIT"
          else
            EFFECTIVE_LIMIT="$LIMIT"
          fi

          # Construye lista base (internas, status<400, no-binarias)
          jq -r '.links[] | select(.status < 400) | .url' crawl.json \
            | awk -v b="$BASE_URL_NO_SLASH" 'index($0,b)==1' \
            | grep -Eiv '\.(pdf|jpg|jpeg|png|gif|svg|webp|mp4|zip|rar|7z|gz|css|js|json|xml)$' \
            | sed 's/#.*$//' \
            | sort -u > urls.all.txt

          # Fallback: si no sali√≥ nada del crawler, intenta sitemap.xml
          if [ ! -s urls.all.txt ]; then
            echo "Crawler vac√≠o; intentando sitemap.xml‚Ä¶"
            curl -fsSL "$BASE_URL_NO_SLASH/sitemap.xml" -o sitemap.xml || true
            if [ -s sitemap.xml ]; then
              grep -oP '(?<=<loc>)[^<]+' sitemap.xml \
                | awk -v b="$BASE_URL_NO_SLASH" 'index($0,b)==1' \
                | sed 's/#.*$//' \
                | sort -u > urls.all.txt
              echo "Sitemap aport√≥ $(wc -l < urls.all.txt) URLs."
            fi
          fi

          if [ "$ANALYZE_ALL" = "true" ]; then
            echo "‚ö†Ô∏è  Modo ANALYZE_ALL activado: se analizar√°n TODAS las URLs encontradas. Puede tardar."
            sed -n '1,$p' urls.all.txt > urls.txt
          else
            START_LINE=$((OFFSET + 1))
            END_LINE=$((OFFSET + EFFECTIVE_LIMIT))
            sed -n "${START_LINE},${END_LINE}p" urls.all.txt > urls.txt
            echo "Lote: OFFSET=$OFFSET  LIMIT=$EFFECTIVE_LIMIT  (l√≠neas ${START_LINE}..${END_LINE})"
          fi

          echo "Total URLs descubiertas: $(wc -l < urls.all.txt)"
          echo "URLs en este lote: $(wc -l < urls.txt)"

          # Pasa a JSON para la matriz
          jq -Rs 'split("\n") | map(select(length>0))' urls.txt > urls.json
      - name: "Count all discovered URLs"
        id: count_all
        run: |
          TOTAL=$(wc -l < urls.all.txt || echo 0)
          echo "total_urls=$TOTAL" >> "$GITHUB_OUTPUT"
      - name: "Prepare matrix for subsequent jobs"
        id: make_matrix
        run: |
          URLS=$(jq -c . urls.json)
          echo "urls=$URLS" >> "$GITHUB_OUTPUT"
      - name: "Upload discovered URLs as artifact"
        uses: actions/upload-artifact@v4
        with:
          name: discovered-urls
          path: |
            urls.txt
            urls.all.txt

  # Lighthouse job, now powered by the matrix
  lighthouse:
    needs: discover-urls
    if: needs.discover-urls.outputs.url_matrix != '[]' && needs.discover-urls.outputs.url_matrix != ''
    name: "Test de Performance (Lighthouse)"
    runs-on: ubuntu-latest
    timeout-minutes: 15
    strategy:
      matrix:
        url: ${{ fromJson(needs.discover-urls.outputs.url_matrix || '[]') }}
      fail-fast: false
      max-parallel: 3 # Control concurrency
    steps:
      - uses: actions/setup-node@v4
        with:
          node-version: 20
      - run: npm install -g @lhci/cli
      - name: "Run Lighthouse CI on ${{ matrix.url }}"
        env:
          LHCI_BUILD_CONTEXT__CURRENT_HASH: ${{ github.sha }}
        run: |
          lhci autorun \
            --collect.url="${{ matrix.url }}" \
            --collect.numberOfRuns=1 \
            --upload.target=temporary-public-storage
      - name: "Upload Lighthouse artifact"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-${{ strategy.job-index }}
          path: .lighthouseci/
          if-no-files-found: ignore

  # Pa11y job, now powered by the matrix
  a11y:
    needs: discover-urls
    if: needs.discover-urls.outputs.url_matrix != '[]' && needs.discover-urls.outputs.url_matrix != ''
    name: "Test de Accesibilidad (Pa11y)"
    runs-on: ubuntu-latest
    timeout-minutes: 12
    strategy:
      matrix:
        url: ${{ fromJson(needs.discover-urls.outputs.url_matrix || '[]') }}
      fail-fast: false
      max-parallel: 3 # Control concurrency
    steps:
      - uses: actions/setup-node@v4
        with:
          node-version: 20
      - run: npm install -g pa11y-ci
      - name: "Run Pa11y on ${{ matrix.url }}"
        run: |
          URL="${{ matrix.url }}"
          cat > pa11yci.json <<'EOF'
          {
            "defaults": {
              "timeout": 30000,
              "runners": ["axe"],
              "chromeLaunchConfig": { "args": ["--no-sandbox","--disable-setuid-sandbox"] }
            },
            "urls": ["__URL__"]
          }
          EOF
          sed -i "s|__URL__|${URL}|g" pa11yci.json
          set -o pipefail
          pa11y-ci --config pa11yci.json | tee pa11y-report.txt
      - name: "Upload Pa11y report artifact"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pa11y-${{ strategy.job-index }}
          path: pa11y-report.txt
          if-no-files-found: ignore

  # ZAP Baseline scan (unchanged)
  zap_baseline:
    name: "Test de Seguridad Basico (ZAP)"
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: "Sanity check BASE_URL"
        run: |
          if [ -z "${{ vars.BASE_URL }}" ]; then
            echo "Error: The BASE_URL variable is not defined."
            exit 1
          fi
      - uses: zaproxy/action-baseline@v0.14.0
        with:
          target: "${{ vars.BASE_URL }}"
          fail_action: false
          allow_issue_writing: false

  # WPScan using official Docker image
  wpscan:
    name: "Test de WordPress (WPScan)"
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: "Sanity check BASE_URL"
        run: |
          if [ -z "${{ vars.BASE_URL }}" ]; then
            echo "Error: The BASE_URL variable is not defined."
            exit 1
          fi

      # üîÄ Toggle WPScan desde variable de repo
      - name: "WPScan toggle (repo var)"
        run: |
          if [ "${{ vars.WPSCAN_ENABLED || 'true' }}" != "true" ]; then
            echo "WPScan desactivado via WPSCAN_ENABLED!=true. Saliendo del job sin ejecutar el escaneo."
            exit 0
          fi

      - name: "Check if target is WordPress (and accessible)"
        if: ${{ vars.WPSCAN_ENABLED == 'true' }}
        id: check_wp
        run: |
          BASE="${{ vars.BASE_URL }}"; BASE="${BASE%/}"
          CODE=$(curl -sL -A "Mozilla/5.0" --retry 2 -o /tmp/index.html -w "%{http_code}" --max-time 15 "$BASE/" || echo "000")
          if [ ! -s /tmp/index.html ]; then
            echo "is_wordpress=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          if [ "$CODE" = "401" ] || [ "$CODE" = "403" ] || ! grep -qi 'wp-content' /tmp/index.html; then
            echo "is_wordpress=false" >> $GITHUB_OUTPUT
          else
            echo "is_wordpress=true" >> $GITHUB_OUTPUT
          fi

      - name: "Run WPScan (official Docker image)"
        if: ${{ vars.WPSCAN_ENABLED == 'true' && steps.check_wp.outputs.is_wordpress == 'true' }}
        env:
          WPSCAN_API_TOKEN: ${{ secrets.WPSCAN_API_TOKEN }}
        run: |
          if [ -z "$WPSCAN_API_TOKEN" ]; then
            echo "WPSCAN_API_TOKEN not found. Skipping WPScan."
            exit 0
          fi
          set -o pipefail
          docker run --rm wpscanteam/wpscan \
            --url "${{ vars.BASE_URL }}" \
            --stealthy \
            --ignore-main-redirect \
            --format cli \
            --api-token "$WPSCAN_API_TOKEN" | tee wpscan-report.txt

      - name: "Upload WPScan report artifact"
        if: ${{ always() && vars.WPSCAN_ENABLED == 'true' && steps.check_wp.outputs.is_wordpress == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: wpscan-report
          path: wpscan-report.txt
          if-no-files-found: ignore

  generate-report:
    name: "Paso Final: Generar Reporte Unificado"
    needs: [discover-urls, lighthouse, a11y, zap_baseline, wpscan]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: "Download all artifacts"
        uses: actions/download-artifact@v4
        with:
          path: reports

      - uses: actions/download-artifact@v4
        with:
          name: discovered-urls
          path: reports/discovered-urls

      - name: "Build full-report.md"
        run: |
          echo "" > full-report.md

          echo "---" >> full-report.md
          echo "## Lighthouse" >> full-report.md
          if ls reports/lighthouse-*/.lighthouseci/*.html >/dev/null 2>&1; then
            for f in reports/lighthouse-*/.lighthouseci/*.html; do
              echo "### Archivo: $f" >> full-report.md
              echo "_(HTML embebido no se muestra en Markdown; abre el artefacto para verlo)_" >> full-report.md
              echo "" >> full-report.md
            done
          else
            echo "Sin artefactos de Lighthouse." >> full-report.md
          fi

          echo "---" >> full-report.md
          echo "## Accesibilidad (Pa11y)" >> full-report.md
          if ls reports/pa11y-*/pa11y-report.txt >/dev/null 2>&1; then
            for f in reports/pa11y-*/pa11y-report.txt; do
              echo "### Archivo: $f" >> full-report.md
              echo '```' >> full-report.md
              cat "$f" >> full-report.md
              echo '```' >> full-report.md
              echo "" >> full-report.md
            done
          else
            echo "Sin artefactos de Pa11y." >> full-report.md
          fi

          echo "---" >> full-report.md
          echo "## Seguridad (ZAP)" >> full-report.md
          if [ -f reports/zap_scan/report_md.md ]; then
            cat reports/zap_scan/report_md.md >> full-report.md
          else
            echo "Sin artefacto de ZAP." >> full-report.md
          fi
          echo ""

          echo "---" >> full-report.md
          echo "## WordPress (WPScan)" >> full-report.md
          if [ -f reports/wpscan-report/wpscan-report.txt ]; then
            echo '```' >> full-report.md
            cat reports/wpscan-report/wpscan-report.txt >> full-report.md
            echo '```' >> full-report.md
          else
            echo "El an√°lisis de WPScan no se ejecut√≥ o no gener√≥ artefacto." >> full-report.md
          fi

          {
            echo "# Reporte de Auditor√≠a QA Completo"
            echo "Fecha: $(date)"
            echo
            echo "## Resumen de descubrimiento"
            echo "- URLs descubiertas (totales): ${{ needs.discover-urls.outputs.total_urls }}"
            echo "- analyze_all: ${{ inputs.analyze_all }}"
            echo "- url_offset: ${{ inputs.url_offset }}"
            echo "- url_limit: ${{ inputs.url_limit }}"
            echo
            echo "## URLs Analizadas (lote actual)"
            if [ -f reports/discovered-urls/urls.txt ]; then nl -ba reports/discovered-urls/urls.txt; else echo "(sin artefacto)"; fi
            echo
          } > full-report.md.tmp

          # concatena el resto del contenido que ya generabas al full-report
          cat full-report.md >> full-report.md.tmp 2>/dev/null || true
          mv -f full-report.md.tmp full-report.md
      - name: "Upload consolidated report"
        uses: actions/upload-artifact@v4
        with:
          name: full-qa-report
          path: full-report.md

